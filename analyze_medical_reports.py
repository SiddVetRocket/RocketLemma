# --- analyze_medical_reports.py ---
"""
Analyze medical reports and extract MeSH-based conditions and findings.

Usage:
    python analyze_medical_reports.py input.csv output.json [limit]

    input.csv   : CSV with at least ID, Findings, Conclusion columns
    output.json : JSON array of per-report results
    limit       : optional integer, number of rows to process (e.g. 200)
"""

import sys
import json
import time
from pathlib import Path

import pandas as pd
import spacy
from spacy.matcher import PhraseMatcher

from mesh_term_loader import load_mesh_term_file


# -----------------------------------------------------------------------------
# Configuration
# -----------------------------------------------------------------------------

BASE_DIR = Path(__file__).resolve().parent

# MeSH term lists generated by mesh_terms_extract.py
MESH_COND_FILE = BASE_DIR / "out" / "conditions_mesh.txt"
MESH_FIND_FILE = BASE_DIR / "out" / "findings_mesh.txt"

# Toggle whether to include synonyms in the phrase matcher
USE_SYNONYMS = True

# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Simple context cues for negation / uncertainty around a hit
UNCERTAIN_PHRASES = [
    "suspected",
    "suspicious for",
    "cannot exclude",
    "cannot rule out",
    "may represent",
    "possibly",
    "possible",
    "likely vs",
    "question of",
]

NEGATION_PHRASES = [
    "no ",
    "without ",
    "free of",
    "rule out",
    "unlikely",
    "no evidence of",
]


# -----------------------------------------------------------------------------
# Build MeSH-based matchers
# -----------------------------------------------------------------------------

def build_matchers(use_synonyms: bool = True):
    """
    Build spaCy PhraseMatchers for conditions and findings
    using MeSH-derived term lists.
    """
    if not MESH_COND_FILE.exists() or not MESH_FIND_FILE.exists():
        raise FileNotFoundError(
            f"Missing MeSH term files.\n"
            f"Expected:\n  {MESH_COND_FILE}\n  {MESH_FIND_FILE}\n"
            f"Run: python mesh_terms_extract.py"
        )

    cond_alias, cond_syns = load_mesh_term_file(str(MESH_COND_FILE))
    find_alias, find_syns = load_mesh_term_file(str(MESH_FIND_FILE))

    cond_matcher = PhraseMatcher(nlp.vocab, attr="LOWER")
    find_matcher = PhraseMatcher(nlp.vocab, attr="LOWER")

    if use_synonyms:
        cond_terms = list(cond_alias.keys())
        find_terms = list(find_alias.keys())
    else:
        # Only preferred terms (canonical)
        cond_terms = list(cond_syns.keys())
        find_terms = list(find_syns.keys())

    cond_patterns = [nlp.make_doc(t) for t in cond_terms]
    find_patterns = [nlp.make_doc(t) for t in find_terms]

    if cond_patterns:
        cond_matcher.add("CONDITION", cond_patterns)
    if find_patterns:
        find_matcher.add("FINDING", find_patterns)

    return cond_matcher, find_matcher, cond_alias, find_alias


cond_matcher, find_matcher, cond_alias, find_alias = build_matchers(USE_SYNONYMS)


# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------

def classify_status(text_lower: str, start_char: int, end_char: int) -> str:
    """
    Classify the status of a hit based on a local text window:
    returns "present", "absent", or "unknown".
    """
    window_start = max(0, start_char - 40)
    window_end = min(len(text_lower), end_char + 40)
    window = text_lower[window_start:window_end]

    # Strong negation -> absent
    for phrase in NEGATION_PHRASES:
        if phrase in window:
            return "absent"

    # Hedging / uncertainty -> unknown
    for phrase in UNCERTAIN_PHRASES:
        if phrase in window:
            return "unknown"

    # Otherwise treat as present
    return "present"


def extract_conditions_and_findings(text: str):
    """
    Run spaCy + MeSH matchers over the text and return
    lists of conditions and findings, each with status.
    """
    doc = nlp(text)
    text_lower = doc.text.lower()

    conditions = []
    findings = []

    seen_cond_spans = set()
    seen_find_spans = set()

    # CONDITIONS
    for match_id, start, end in cond_matcher(doc):
        span = doc[start:end]
        alias = span.text.lower()
        canonical = cond_alias.get(alias, alias)

        key = (span.start_char, span.end_char, canonical)
        if key in seen_cond_spans:
            continue
        seen_cond_spans.add(key)

        status = classify_status(text_lower, span.start_char, span.end_char)

        conditions.append({
            "text": span.text,
            "canonical": canonical,
            "start": span.start_char,
            "end": span.end_char,
            "status": status,
            "source": "mesh",
        })

    # FINDINGS
    for match_id, start, end in find_matcher(doc):
        span = doc[start:end]
        alias = span.text.lower()
        canonical = find_alias.get(alias, alias)

        key = (span.start_char, span.end_char, canonical)
        if key in seen_find_spans:
            continue
        seen_find_spans.add(key)

        status = classify_status(text_lower, span.start_char, span.end_char)

        findings.append({
            "text": span.text,
            "canonical": canonical,
            "start": span.start_char,
            "end": span.end_char,
            "status": status,
            "source": "mesh",
        })

    return conditions, findings


# -----------------------------------------------------------------------------
# Main: CSV -> JSON
# -----------------------------------------------------------------------------

def main():
    if len(sys.argv) < 3:
        print("Usage: python analyze_medical_reports.py input.csv output.json [limit]")
        sys.exit(1)

    input_csv = Path(sys.argv[1])
    output_json = Path(sys.argv[2])
    limit = int(sys.argv[3]) if len(sys.argv) > 3 else None

    if not input_csv.exists():
        print(f"Input CSV not found: {input_csv}")
        sys.exit(1)

    df = pd.read_csv(input_csv)
    if limit is not None:
        df = df.head(limit)

    results = []
    t0 = time.perf_counter()

    for idx, row in df.iterrows():
        # Adjust column names here if yours differ
        findings_text = str(row.get("Findings", "") or "")
        conclusion_text = str(row.get("Conclusion", "") or "")

        text = " ".join(part for part in [findings_text, conclusion_text] if part)

        conditions, findings = extract_conditions_and_findings(text)

        result = {
            "id": row.get("ID"),
            "conditions": conditions,
            "findings": findings,
        }

        results.append(result)

    elapsed = time.perf_counter() - t0
    print(f"Processed {len(results)} reports in {elapsed:.2f} seconds")

    with output_json.open("w", encoding="utf-8") as out_f:
        json.dump(results, out_f, indent=2)

    print(f"Wrote {output_json}")


if __name__ == "__main__":
    main()
